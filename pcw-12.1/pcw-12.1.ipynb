{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bathientran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(get_stop_words('en')) \n",
    "nltk_words = list(stopwords.words('english'))\n",
    "stop_words.extend(nltk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"odyssey.txt\", \"r\")\n",
    "odyssey = f.read().translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "text_list = odyssey.replace('\\n', ' ').replace('”','').replace('“','')\n",
    "text_list = text_list.split('BOOK')\n",
    "for i in range(1,len(text_list)):\n",
    "    text_list[i] = text_list[i][5:].lower()\n",
    "    text_list[i] = text_list[i].split(' ')\n",
    "    text_list[i] = list(filter(None, text_list[i]))\n",
    "    text_list[i] = [w for w in text_list[i] if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted from https://medium.com/analytics-vidhya/topic-modelling-using-latent-dirichlet-allocation-in-scikit-learn-7daf770406c4\n",
    "lda = LatentDirichletAllocation(n_components=10)\n",
    "cv = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')\n",
    "for i in range(1,len(text_list)):\n",
    "    df = cv.fit_transform(text_list[i])\n",
    "    lda.fit(df)\n",
    "    transformed_text = lda.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for Topic #1\n",
      "['know', 'right', 'told', 'peleus', 'penelope', 'away', 'make', 'work', 'left', 'minerva']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #2\n",
      "['peace', 'achilles', 'far', 'fighting', 'brought', 'way', 'hands', 'dolius', 'took', 'man']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #3\n",
      "['country', 'presents', 'killed', 'wife', 'ground', 'say', 'mother', 'suitors', 'shall', 'round']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #4\n",
      "['people', 'ready', 'heaven', 'end', 'ghost', 'fell', 'armour', 'laertes', 'tell', 'son']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #5\n",
      "['saw', 'gathered', 'began', 'home', 'got', 'ghosts', 'telemachus', 'gave', 'ithaca', 'said']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #6\n",
      "['patroclus', 'things', 'matter', 'better', 'heard', 'garden', 'dinner', 'sons', 'come', 'went']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #7\n",
      "['disgrace', 'bitterly', 'town', 'sea', 'let', 'great', 'jove', 'father', 'came', 'ulysses']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #8\n",
      "['spear', 'wept', 'close', 'fear', 'fair', 'time', 'gone', 'saying', 'fallen', 'house']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #9\n",
      "['eupeithes', 'voice', 'long', 'spoke', 'place', 'laid', 'agamemnon', 'death', 'answered', 'old']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #10\n",
      "['ships', 'troy', 'soon', 'fine', 'return', 'stood', 'achaeans', 'good', 'gods', 'men']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing components\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Top 10 words for Topic #{index+1}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that this is wrong because I wasn't sure how I can train the model with different parts of the text (partially due to transformation). I used the CountVectorizer to process each chapter and then fit the LDA to the given chapter. Presumably, it should be fit with all chapters at the same time but I'm not sure how the data should look like to achieve this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
